---
title: 如何理解特征工程
date: 2019-4-24 17:29:12
categories:
    - 机器学习
tags: 
    - 数据处理
mathjax: true
---

书接上回，从[Kaggle-泰坦尼克号之灾](https://crowfeablog.com/article/Kaggle-%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%8B%E7%81%BE/)中我们提到了特征工程。

> 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。

上面是江湖上流传的一句话，哪位大神说的亦不可考。但可见特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。我们也参加过建模等比赛，每个竞赛的好成绩其实并没有用到很高深的算法，大多数都是在特征工程这个环节做出了出色的工作，然后使用一些常见的算法，比如LR，就能得到出色的性能。

理解特征工程应当从以下几个角度出发：

*   特征工程是什么？
*   为什么要做特征工程？
*   应该如何做特征工程？

接下类我们详细论述一下。

### 什么是特征工程

当我们对数据建立模型分析的时候，首先要干的事情是对数据进行处理和分析。没有数据就没有办法进行模型的建立，那么拿到什么数据就显得至关重要。很显然，怎么对原始数据进行处理，就是非常重要的一步。

特征工程干的，就是这一步的工作。简而言之，特征工程就是一个把原始数据转变成特征的过程，这些特征可以很好的描述这些数据，并且利用它们建立的模型在未知数据上的表现性能可以达到最优（或者接近最佳性能）。从数学的角度来看，特征工程就是人工地去设计输入变量X。

简而言之，特征工程的目的，在于**获得更容易表现数据性质的训练集**。

### 为什么要做特征工程

OK！知道了特征工程是什么，那么我们必须要来了解下特征工程的重要性，为什么在实际工作中都要有特征工程这个过程，下面不同的角度来分析一下。

首先，我们大家都知道，数据特征会直接影响我们模型的预测性能。你可以这么说：“选择的特征越好，最终得到的性能也就越好”。这句话说得没错，但也会给我们造成误解。事实上，你得到的实验结果取决于你选择的模型、获取的数据以及使用的特征，甚至你问题的形式和你用来评估精度的客观方法也扮演了一部分。此外，你的实验结果还受到许多相互依赖的属性的影响，你需要的是能够很好地描述你数据内部结构的好特征。

*   特征越好，灵活性越强

只要特征选得好，即使是一般的模型（或算法）也能获得很好的性能，因为大多数模型（或算法）在好的数据特征下表现的性能都还不错。好特征的灵活性在于它允许你选择不复杂的模型，同时运行速度也更快，也更容易理解和维护。

*   特征越好，构建的模型越简单

有了好的特征，即便你的参数不是最优的，你的模型性能也能仍然会表现的很好，所以你就不需要花太多的时间去寻找最优参数，这大大的降低了模型的复杂度，使模型趋于简单。

*   特征越好，模型的性能越出色

显然，这一点是毫无争议的，我们进行特征工程的最终目的就是提升模型的性能。

### 如何做特征工程

> Feature engineering is a super-set of activities which include feature extraction, feature construction and feature selection. Each of the three are important steps and none should be ignored. We could make a generalization of the importance though, from my experience the relative importance of the steps would be feature construction &gt; feature extraction &gt; feature selection.——Quora
> 
> 特征工程是一个超集，它包括特征提取、特征构建和特征选择这三个子模块。在实践当中，每一个子模块都非常重要，忽略不得。根据答主的经验，他将这三个子模块的重要性进行了一个排名，即：特征构建&gt;特征提取&gt;特征选择。

特征工程并非是一个问题。他的操作步骤主要包括：Feature Selection（特征选择）、Feature Extraction（特征提取）和Feature construction（特征构造）。

*   特征选择：它的目的是从特征集合中挑选一组最具统计意义的特征子集，从而达到降维的效果。
*   特征提取：对象是原始数据（raw data），它的目的是自动地构建新的特征，将原始特征转换为一组具有明显物理意义（Gabor、几何特征[角点、不变量]、纹理[LBP HOG]）或者统计意义或核的特征。
*   特征构建：指的是从原始数据中人工的构建新的特征。

对于一个实际的的机器学习过程来说，一般有如下几个阶段：

1.（Task before here）

2.选择数据(Select Data): 整合数据，将数据规范化成一个数据集，收集起来.

3.数据预处理（Preprocess Data）: 数据格式化，数据清理，采样等.

4.数据转换（Transform Data）: 这个阶段做特征工程.

5.数据建模（Model Data）: 建立模型，评估模型并逐步优化.

(Tasks after here…)

我们发现，特征工程和数据转换其实是等价的。事实上，特征工程是一个迭代过程，我们需要不断的设计特征、选择特征、建立模型、评估模型，然后才能得到最终的model。

我们接下来主要讲一下特征处理。

### 特征处理

特征处理是一个比较复杂的工作，包括清洗数据、预处理、降维、选择。是上述的特征工程三部贯通的一些方法和思路。对于不同类型的数据，会有不一样的处理。

*   数据清洗

数据清洗的意思为**消除异常数据**。现实生活中会出现很多意想不到的情况，在统计意义上来说可忽略，我们希望数据尽可能地表现出统计的特征，所以要清除掉一些异常的数据。

我们将在另一篇文章中详细的介绍数据清洗的方法。

[机器学习-数据清洗方法论](https://crowfeablog.com/article/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E6%96%B9%E6%B3%95%E8%AE%BA/)

*   数据预处理

即使清洗完了数据，我们的数据仍可能是非常不规范的。并且各种格式的都有，接下来要干的事情就是数据的预处理。在处理的过程中，我们会面对各种类型的数据，比如数值型（、类别型（比如某个品牌的口红可能有十八种色号，比如衣服大小L XL XLL，比如星期几）、时间类、文本型、统计型、组合特征等等。

如果我们对于模型有基本的想法，那么对于不同的模型所要求的数据也是不一样的。比如LR会喜欢离散的数据，所以在建模之前要将数据离散化；通过梯度下降法求解的模型一般都是需要归一化的，也是要进行处理。

同样，介绍数据预处理也会有一篇文章（真的挖了好多坑）。

*   特征选择

在实际项目中，并不是特征维数越高越好。

特征维数越高，模型越容易过拟合，此时更复杂的模型就不好用。相互独立的特征维数越高，在模型不变的情况下，在测试集上达到相同的效果表现所需要的训练样本的数目就越大。

特征数量增加带来的训练、测试以及存储的开销都会增大。

![维度灾难](https://s2.ax1x.com/2019/04/24/EZGeDs.jpg)

在某些模型中，例如基于距离计算的模型KMeans，KNN等模型，在进行距离计算时，维度过高会影响精度和性能。5.可视化分析的需要。在低维的情况下，例如二维，三维，我们可以把数据绘制出来，可视化地看到数据。当维度增高时，就难以绘制出来了。

如何选择特征请看（坑）

### 总结

这篇文章对特征工程有一个总览，对于其中详细的方法会在另外三篇文章中进行阐述。

                